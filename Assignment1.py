# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p8wgVGJv3lnNkfmh8t4VVg1X0wJdrhdn
"""

import numpy as np

np.random.seed(0)
array = np.random.randint(1, 51, size=(5, 4))

print("Original Array:\n", array)
anti_diagonal = [array[i, -1 - i] for i in range(min(array.shape))]
print("Anti-diagonal elements:", anti_diagonal)
row_max = np.max(array, axis=1)
print("Maximum value in each row:", row_max)
mean_val = np.mean(array)
filtered_array = array[array <= mean_val]
print("Elements less than or equal to the mean (mean =", mean_val, "):", filtered_array)
def numpy_boundary_traversal(matrix):
    top = list(matrix[0, :])
    right = list(matrix[1:-1, -1])
    bottom = list(matrix[-1, ::-1])
    left = list(matrix[-2:0:-1, 0])
    return top + right + bottom + left

boundary_elements = numpy_boundary_traversal(array)
print("Boundary traversal in clockwise order:", boundary_elements)

np.random.seed(1)
array = np.random.uniform(0, 10, 20)
rounded_array = np.round(array, 2)
print("Original Array (rounded):", rounded_array)
min_val = np.min(array)
max_val = np.max(array)
median_val = np.median(array)
print("Minimum:", round(min_val, 2))
print("Maximum:", round(max_val, 2))
print("Median:", round(median_val, 2))
modified_array = np.where(array < 5, np.round(array**2, 2), np.round(array, 2))
print("Array with elements < 5 squared:", modified_array)
def numpy_alternate_sort(array):
    sorted_array = np.sort(array)
    result = []
    i, j = 0, len(sorted_array) - 1
    while i <= j:
        result.append(sorted_array[i])
        if i != j:
            result.append(sorted_array[j])
        i += 1
        j -= 1
    return np.array(result)

alt_sorted = numpy_alternate_sort(array)
print("Alternate Sorted Array:", np.round(alt_sorted, 2))

import pandas as pd

names = ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank', 'Grace', 'Helen', 'Ian', 'Julia']
subjects = ['Math', 'Science', 'English', 'Math', 'Science', 'English', 'Math', 'Science', 'English', 'Math']

np.random.seed(2)
scores = np.random.randint(50, 101, size=10)
df = pd.DataFrame({'Name': names, 'Subject': subjects, 'Score': scores})

def assign_grade(score):
    if score >= 90:
        return 'A'
    elif score >= 80:
        return 'B'
    elif score >= 70:
        return 'C'
    elif score >= 60:
        return 'D'
    else:
        return 'F'

df['Grade'] = df['Score'].apply(assign_grade)

sorted_df = df.sort_values(by='Score', ascending=False)
print("Sorted DataFrame by Score (descending):\n", sorted_df)

avg_scores = df.groupby('Subject')['Score'].mean()
print("\nAverage Score by Subject:\n", avg_scores)

def pandas_filter_pass(dataframe):
    return dataframe[dataframe['Grade'].isin(['A', 'B'])]

filtered_df = pandas_filter_pass(df)
print("\nFiltered DataFrame (Grade A or B):\n", filtered_df)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

positive_reviews = ['Loved it!', 'Amazing movie', 'Great acting', 'Awesome plot', 'Superb direction',
                    'Fantastic film', 'Brilliant!', 'Heartwarming', 'Touching and beautiful', 'Very enjoyable'] * 5
negative_reviews = ['Hated it.', 'Terrible movie', 'Awful acting', 'Boring plot', 'Poor direction',
                    'Horrible film', 'Waste of time', 'Disappointing', 'Dull and forgettable', 'Not enjoyable'] * 5

reviews = positive_reviews + negative_reviews
sentiments = ['positive'] * 50 + ['negative'] * 50

df = pd.DataFrame({'Review': reviews, 'Sentiment': sentiments})

vectorizer = CountVectorizer(max_features=500, stop_words='english')
X = vectorizer.fit_transform(df['Review'])
y = df['Sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", round(accuracy * 100, 2), "%")

def predict_review_sentiment(model, vectorizer, review):
    review_vector = vectorizer.transform([review])
    prediction = model.predict(review_vector)
    return prediction[0]

example_review = "It was an amazing movie with great performances!"
predicted_sentiment = predict_review_sentiment(model, vectorizer, example_review)
print("Predicted Sentiment for sample review:", predicted_sentiment)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score

good_reviews = ['Excellent product', 'Loved it', 'Very useful', 'Highly recommend', 'Good quality',
                'Amazing experience', 'Great value', 'Perfect buy', 'Very satisfied', 'Outstanding'] * 5
bad_reviews = ['Terrible product', 'Hated it', 'Not useful', 'Do not recommend', 'Poor quality',
               'Awful experience', 'Waste of money', 'Worst purchase', 'Not satisfied', 'Disappointing'] * 5

texts = good_reviews + bad_reviews
labels = ['good'] * 50 + ['bad'] * 50

df = pd.DataFrame({'Text': texts, 'Label': labels})

vectorizer = TfidfVectorizer(max_features=300, stop_words='english', lowercase=True)
X = vectorizer.fit_transform(df['Text'])
y = df['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

precision = precision_score(y_test, y_pred, pos_label='good')
recall = recall_score(y_test, y_pred, pos_label='good')
f1 = f1_score(y_test, y_pred, pos_label='good')

print(f"Precision: {precision:.2f}")
print(f"Recall:    {recall:.2f}")
print(f"F1-Score:  {f1:.2f}")

def text_preprocess_vectorize(texts, vectorizer):
    return vectorizer.transform(texts)

#eg
new_texts = ["I love this product", "Worst experience ever"]
vectorized_texts = text_preprocess_vectorize(new_texts, vectorizer)
predictions = model.predict(vectorized_texts)
print("Predictions for new samples:", predictions)

