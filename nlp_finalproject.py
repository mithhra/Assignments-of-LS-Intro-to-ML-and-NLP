# -*- coding: utf-8 -*-
"""NLP.FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M8s2cPBvLPY_pL_-iG5bsOqbmkbUuzOX
"""

!pip install transformers datasets evaluate gradio --quiet

!pip install transformers datasets evaluate

from datasets import load_dataset
from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
import math
import gradio as gr
from sklearn.model_selection import train_test_split
import torch

import requests

shakespeare_url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
text = requests.get(shakespeare_url).text

train_text, val_text = train_test_split(text.split('\n'), test_size=0.05)
train_text = "\n".join(train_text)
val_text = "\n".join(val_text)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def tokenize_text(text):
    tokens = tokenizer(text, return_special_tokens_mask=True, truncation=True)
    return {"input_ids": tokens["input_ids"]}

train_encodings = tokenize_text(train_text)
val_encodings = tokenize_text(val_text)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, block_size=128):
        self.input_ids = encodings["input_ids"]
        total_length = (len(self.input_ids) // block_size) * block_size
        self.blocks = [
            torch.tensor(self.input_ids[i:i+block_size])
            for i in range(0, total_length, block_size)
        ]

    def __len__(self):
        return len(self.blocks)

    def __getitem__(self, idx):
        return {"input_ids": self.blocks[idx], "labels": self.blocks[idx]}

train_dataset = TextDataset(train_encodings)
val_dataset = TextDataset(val_encodings)

model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    report_to="none",
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

trainer.train()

eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

def predict_next_word(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

gr.Interface(fn=predict_next_word, inputs="text", outputs="text", title="Next Word Predictor").launch()